# 三.软件安装

## 安装JAVA（所有节点）

    `yum -y install java java-devel`

1. 查看java版本，确保此命令没有问题

    `java -version`

    openjdk version "1.8.0_161"
    OpenJDK Runtime Environment (build 1.8.0_161-b14)
    OpenJDK 64-Bit Server VM (build 25.161-b14, mixed mode)

2. 另外`openjdk`安装后，不会默许设置`JAVA_HOME`环境变量

- 查看安装后的目录

 `update-alternatives --config jre_openjdk`

  默认jre目录为：`/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.161-0.b14.el7_4.x86_64/jre`

- 设置环境变量
 `vim /etc/profile.d/java.sh`

  ```bash
    #!/bin/bash
    #
    export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.161-0.b14.el7_4.x86_64
    export CLASSPATH=$JAVA_HOME/lib/rt.jar:$JAVA_HOME/../lib/dt.jar:$JAVA_HOME/../lib/tools.jar
    export PATH=$PATH:$JAVA_HOME/bin
  ```

  可以使用 echo 写入 ，也可使用 touch 和 sed 写入

- 使环境变量生效

 `source /etc/profile.d/java.sh2`

-----

## 创建hadoop用户（所有节点）

1. 创建

 `useradd hadoop -p 123456`

 > 设置密码，为简单起见，3台机器上的`hadoop`密码最好设置成一样，比如`123456`

2. 为了方便，建议将hadoop加入root用户组，

 `usermod -g root hadoop`

 执行完后hadoop即归属于root组了

 > 也可以直接 `useradd -g root -p 123456 adoop`

3. 可以再输入id hadoop查看输出验证一下，如果看到类似下面的输出：

 `id hadoop`

 `uid=1002(hadoop) gid=0(root) groups=0(root)`

-----

## 在NameNode节点创建秘钥 

1. 192.168.10.90：

创建RSA秘钥对
`su hadoop`
`ssh-keygen`

> 此时可能会没有家目录，可以创建，并chown赋予权限
> 成功的话会输出 SHA256 的一个加密key finger

2. 复制公钥到所有节点Hadoop用户目录下，包括自己：

```bash
ssh-copy-id hadoop@192.168.10.90
ssh-copy-id hadoop@192.168.10.91
ssh-copy-id hadoop@192.168.10.92
```

> 会需要输入密码

## 解压Hadoop二进制包并设置环境变量（所有节点）

```bash
wget http://mirror.bit.edu.cn/apache/hadoop/common/hadoop-2.8.4/hadoop-2.8.4.tar.gz
tar xf hadoop-2.8.4.tar.gz -C /usr/local/
ln -sv /usr/local/hadoop-2.8.4/ /usr/local/hadoop
```

1. 编辑环境配置文件
   `/etc/profile.d/hadoop.sh`

定义类似如下环境变量，设定Hadoop的运行环境。

```bash
#!/bin/bash
#
export HADOOP_PREFIX="/usr/local/hadoop"
export PATH=$PATH:$HADOOP_PREFIX/bin:$HADOOP_PREFIX/sbin
export HADOOP_COMMON_HOME=${HADOOP_PREFIX}
export HADOOP_HDFS_HOME=${HADOOP_PREFIX}
export HADOOP_MAPRED_HOME=${HADOOP_PREFIX}
export HADOOP_YARN_HOME=${HADOOP_PREFIX}
```

2. 创建数据和日志目录：

```bash
mkdir -pv /data/hadoop/hdfs/{master,dn-01,dn-02}
chown -R hadoop:hadoop /data/hadoop/hdfs
mkdir -pv /var/log/hadoop/yarn
chown -R hadoop:hadoop /var/log/hadoop/yarn
```

3. 在Hadoop的安装目录中创建logs目录，并修改Hadoop所有文件的组和用户。

   > 这里的组和用户一定要指定对应之前创建用户时的 **组和用户**

```bash
cd /usr/local/hadoop
mkdir logs
chmod g+w logs
chown -R hadoop:root ./*
```