# 配置文件

## 设置时区

`CELERY_TIMEZONE = 'Asia/Shanghai'`

## 启动时区设置

`CELERY_ENABLE_UTC = True`

## 限制任务的执行频率

- 下面这个就是限制 tasks 模块下的 add 函数，每秒钟只能执行 10 次

`CELERY_ANNOTATIONS = {'tasks.add':{'rate_limit':'10/s'}}`

- 或者限制所有的任务的刷新频率

`CELERY_ANNOTATIONS = {'\*':{'rate_limit':'10/s'}}`

## 也可以设置如果任务执行失败后调用的函数

```py
def my_on_failure(self,exc,task_id,args,kwargs,einfo):
    print('task failed')

CELERY_ANNOTATIONS = {'\*':{'on_failure':my_on_failure}}
```

## 并发的 worker 数量，也是命令行-c 指定的数目

事实上并不是 worker 数量越多越好，保证任务不堆积，加上一些新增任务的预留就可以了

`CELERYD_CONCURRENCY = 20`

## celery worker 每次去 redis 取任务的数量，默认值就是 4

`CELERYD_PREFETCH_MULTIPLIER = 4`

## 每个 worker 执行了多少次任务后就会死掉，建议数量大一些

`CELERYD_MAX_TASKS_PER_CHILD = 200`

## 使用 redis 作为任务队列

组成: `db+scheme://user:password@host:port/dbname`
`BROKER_URL = 'redis://127.0.0.1:6379/0'`

## celery 任务执行结果的超时时间

`CELERY_TASK_RESULT_EXPIRES = 1200`

## 单个任务的运行时间限制，否则会被杀死

`CELERYD_TASK_TIME_LIMIT = 60`

## 使用 redis 存储任务执行结果，默认不使用

`CELERY_RESULT_BACKEND = 'redis://127.0.0.1:6379/1'`

## 将任务结果使用'pickle'序列化成'json'格式

- 任务序列化方式
  `CELERY_TASK_SERIALIZER = 'pickle'`

- 任务执行结果序列化方式

`CELERY_RESULT_SERIALIZER = 'json'`

- 也可以直接在 Celery 对象中设置序列化方式

`app = Celery('tasks', broker='...', task_serializer='yaml')`

## 关闭限速

`CELERY_DISABLE_RATE_LIMITS = True`
